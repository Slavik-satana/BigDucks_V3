{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f006153-2c5c-48aa-8a7d-f5777573b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14ca214-48dd-48d2-8989-7e11759d39be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "valid_df = pd.read_csv('valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8619d33-6110-4821-8442-6d1343827454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    import re\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
    "valid_df['text'] = valid_df['text'].apply(preprocess_text)\n",
    "\n",
    "def balance_dataset(dataframe):\n",
    "    # Calculate the sum of each label across the dataset\n",
    "    label_counts = dataframe.iloc[:, 1:].sum()\n",
    "\n",
    "    # Determine the maximum class size\n",
    "    max_class_size = label_counts.max()\n",
    "\n",
    "    # Create a balanced dataframe\n",
    "    balanced_frames = []\n",
    "    for label in label_counts.index:\n",
    "        label_frame = dataframe[dataframe[label] == 1]\n",
    "        if len(label_frame) > 0:\n",
    "            upsampled_frame = resample(\n",
    "                label_frame,\n",
    "                replace=True,  # Sample with replacement\n",
    "                n_samples=max_class_size,  # Match majority class size\n",
    "                random_state=42\n",
    "            )\n",
    "            balanced_frames.append(upsampled_frame)\n",
    "\n",
    "    balanced_df = pd.concat(balanced_frames)\n",
    "    return balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_df = balance_dataset(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759488c-3cff-44ee-9d40-bf6339984d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa223ec-2ce9-40cb-80fb-6eef50bf13ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.texts = dataframe['text'].tolist()\n",
    "        self.labels = dataframe.iloc[:, 1:].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./results/checkpoint-573\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"./results/checkpoint-573\", \n",
    "    num_labels=7, \n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "train_dataset = EmotionDataset(train_df, tokenizer, MAX_LENGTH)\n",
    "valid_dataset = EmotionDataset(valid_df, tokenizer, MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c2775d-7c0d-45f1-8673-1ef2e6f6b269",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/rosberta/',\n",
    "    eval_strategy=\"epoch\",                           \n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    lr_scheduler_type=\"cosine\", \n",
    "    warmup_steps=300,\n",
    "    save_strategy=\"epoch\", \n",
    "    save_total_limit=2, \n",
    "    save_on_each_node=True,\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    import torch\n",
    "    from sklearn.metrics import f1_score, accuracy_score\n",
    "    logits, labels = pred\n",
    "    # Применяем сигмоиду для получения вероятностей\n",
    "    probabilities = torch.sigmoid(torch.tensor(logits)).numpy()\n",
    "    # Бинаризация предсказаний с порогом 0.5\n",
    "    predictions = (probabilities > 0.5).astype(int)\n",
    "    # Подсчет метрик\n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\", zero_division=0)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\"f1\": f1, \"accuracy\": accuracy}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
