# Классификация текстов по эмоциям на русском языке

Решение от команды 40 (БигДаки) для хакатона MFTIDS_2 x URFUML

## Содержание

1. [Состав команды и роли](#состав-команды-и-роли)
2. [Актуальное состояние проекта](#актуальное-состояние-проекта)
3. [Используемые методы](#используемые-методы)
    * [Предобработка данных](#предобработка-данных)
    * [Архитектура модели](#архитектура-модели)
4. [Планы на будущее](#планы-на-будущее)

---

## Состав команды и роли

- Самаковский Вячеслав: Тим-лид, расширение датасета
- Казиев Владислав: Построение классификатора, расширение датасета
- Гришин Егор: Построение классификатора, расширение датасета
- Алексеев Арслан: Расширение датасета, визуализация результатов

## Актуальное состояние проекта
В настоящий момент удалось набрать score 0.58766 на лидерборде в kaggle

- Протестированные модели:
  - ru-en-RoSBERTa (В настоящий момент показала лучший езультат)
  - xlm-roberta-base
  - rugpt3small_based_on_gpt2
  - ruRoberta-large
  - ru-t5-large

## Используемые методы

### Предобработка данных

- На данном этапе предобработка данных включает базовые шаги. Планируется углубленная очистка данных от шума и ошибок в том числе с помощью ai-forever/sage.

### Архитектура модели

- Используются различные модели трансформеров для поиска оптимальной архитектуры:
  - ru-en-RoSBERTa
  - xlm-roberta-base
  - rugpt3small_based_on_gpt2
  - ruRoberta-large
  - ru-t5-large

## Планы на будущее

В ближайших планах команды:

1. Проверка влияния Stratified K Fold Cross Validation:
   - Для повышения стабильности и обобщающей способности модели.

2. Тестирование различных моделей из лидерборда на русском языке:
   - Исследование моделей с лидерборда HuggingFace на предмет их эффективности для нашей задачи.

3. Дальнейшее расширение датасета:
   - За счёт интеграции других релевантных датасетов, например, ru-go-emotions.

4. Оптимизация предобработки данных:
   - Внедрение методов очистки данных от шума и ошибок для улучшения качества обучения.

